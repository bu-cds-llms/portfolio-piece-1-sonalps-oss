{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "id": "a20b4a98",
      "cell_type": "markdown",
      "source": "# PyTorch Backprop in Action (Single Notebook Version)\n\n**Academic portfolio piece**: build a small neural network in PyTorch, train it on a toy classification dataset, experiment with architectures/activations, and interpret results.\n\nThis notebook is designed to satisfy:\n\n- **Conceptual Understanding (5 pts)** \u2014 clear \u201cwhy\u201d explanations connected to course concepts (gradients/backprop, activations, capacity).\n- **Technical Implementation (5 pts)** \u2014 code runs end-to-end without errors (data \u2192 model \u2192 train \u2192 eval \u2192 outputs).\n- **Code Quality & Documentation (5 pts)** \u2014 clean sections and reusable helper functions (kept inside this notebook).\n- **Critical Analysis (5 pts)** \u2014 compares experiments + discusses limitations/tradeoffs.\n- **Peer Reviews (5 pts)** \u2014 templates at the end.\n\n---",
      "metadata": {}
    },
    {
      "id": "69d27895",
      "cell_type": "markdown",
      "source": "## 0) Setup & imports\n\nWe use:\n- **scikit-learn** to generate a toy dataset (moons/circles)\n- **PyTorch** for model + training loop\n- **matplotlib** for plots\n\nIn a real project we'd split code into `.py` modules, but this version keeps everything inside one notebook (as requested).\n",
      "metadata": {}
    },
    {
      "id": "b2a66f26",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import os, random\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\nprint(\"Torch version:\", torch.__version__)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n",
      "outputs": []
    },
    {
      "id": "db4ebd8c",
      "cell_type": "markdown",
      "source": "## 1) Helper functions (reproducibility, metrics, plotting)\n\n### Why seeds matter\nNeural networks have randomness (initial weights, shuffled batches). Setting seeds makes your results **reproducible**, which is important in both academic and industry settings.\n",
      "metadata": {}
    },
    {
      "id": "57da68a9",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy_from_logits(logits: torch.Tensor, y: torch.Tensor) -> float:\n    preds = torch.argmax(logits, dim=1)\n    return (preds == y).float().mean().item()\n\n\n@torch.no_grad()\ndef plot_dataset(X: np.ndarray, y: np.ndarray, title: str = \"Dataset\") -> None:\n    plt.figure(figsize=(6, 5))\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=10)\n    plt.title(title)\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.grid(True)\n    plt.show()\n\n\n@torch.no_grad()\ndef decision_boundary(model: nn.Module, X: np.ndarray, y: np.ndarray, steps: int = 250, title: str = \"Decision boundary\") -> None:\n    model.eval()\n    x_min, x_max = X[:, 0].min() - 0.75, X[:, 0].max() + 0.75\n    y_min, y_max = X[:, 1].min() - 0.75, X[:, 1].max() + 0.75\n\n    xs = np.linspace(x_min, x_max, steps)\n    ys = np.linspace(y_min, y_max, steps)\n    xx, yy = np.meshgrid(xs, ys)\n    grid = np.stack([xx.ravel(), yy.ravel()], axis=1).astype(np.float32)\n\n    logits = model(torch.from_numpy(grid).to(device))\n    preds = torch.argmax(logits, dim=1).cpu().numpy().reshape(xx.shape)\n\n    plt.figure(figsize=(6, 5))\n    plt.contourf(xx, yy, preds, alpha=0.35)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=10)\n    plt.title(title)\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.grid(True)\n    plt.show()\n\n\ndef plot_history(history: Dict[str, List[float]]) -> None:\n    epochs = list(range(1, len(history[\"train_loss\"]) + 1))\n\n    plt.figure(figsize=(7, 4))\n    plt.plot(epochs, history[\"train_loss\"], label=\"train loss\")\n    plt.plot(epochs, history[\"val_loss\"], label=\"val loss\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss\")\n    plt.title(\"Loss curves\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(7, 4))\n    plt.plot(epochs, history[\"train_acc\"], label=\"train acc\")\n    plt.plot(epochs, history[\"val_acc\"], label=\"val acc\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"accuracy\")\n    plt.title(\"Accuracy curves\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(7, 4))\n    plt.plot(epochs, history[\"grad_norm\"], label=\"avg grad norm\")\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"L2 grad norm\")\n    plt.title(\"Gradient norm (backprop signal)\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n",
      "outputs": []
    },
    {
      "id": "4a3e71b0",
      "cell_type": "markdown",
      "source": "## 2) Create a toy dataset (moons or circles)\n\n### Why a toy 2D dataset?\n- Easy to visualize\n- Great for understanding **non-linear decision boundaries**\n- Quickly shows underfitting/overfitting\n\nWe split into:\n- **train** (learn parameters)\n- **val** (choose best epoch/model)\n- **test** (final unbiased evaluation)\n",
      "metadata": {}
    },
    {
      "id": "117b3d8c",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "seed_everything(42)\n\nDATASET = \"moons\"  # change to \"circles\" if you want\nN_SAMPLES = 2500\nNOISE = 0.25\n\nif DATASET == \"moons\":\n    X, y = make_moons(n_samples=N_SAMPLES, noise=NOISE, random_state=42)\nelif DATASET == \"circles\":\n    X, y = make_circles(n_samples=N_SAMPLES, noise=NOISE, factor=0.5, random_state=42)\nelse:\n    raise ValueError(\"DATASET must be 'moons' or 'circles'\")\n\nX = X.astype(np.float32)\ny = y.astype(np.int64)\n\n# train/test\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n# train/val\nX_train, X_val, y_train, y_val = train_test_split(\n    X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval\n)\n\nprint(\"Shapes:\",\n      \"train\", X_train.shape, y_train.shape,\n      \"val\", X_val.shape, y_val.shape,\n      \"test\", X_test.shape, y_test.shape)\n\nplot_dataset(np.vstack([X_train, X_val, X_test]), np.hstack([y_train, y_val, y_test]),\n             title=f\"Toy dataset: {DATASET} (noise={NOISE})\")\n",
      "outputs": []
    },
    {
      "id": "46a03e7a",
      "cell_type": "markdown",
      "source": "## 3) DataLoaders\n\n### Why DataLoader?\nIt batches data and shuffles training samples, which is the standard PyTorch pattern for training.\n",
      "metadata": {}
    },
    {
      "id": "5862dd8b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "BATCH_SIZE = 128\n\ntrain_loader = DataLoader(\n    TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train)),\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\nval_loader = DataLoader(\n    TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)),\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\ntest_loader = DataLoader(\n    TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test)),\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n",
      "outputs": []
    },
    {
      "id": "5aa061ba",
      "cell_type": "markdown",
      "source": "## 4) Model: MLP (Multi-Layer Perceptron)\n\n### Why MLP?\nA linear model can't solve moons/circles well because the boundary is non-linear.\nAn MLP with hidden layers + activation functions can learn complex boundaries.\n\n### Why activations matter\n- **ReLU**: popular, trains fast, avoids vanishing gradients (often)\n- **Tanh**: smooth but can saturate (vanishing gradients)\n- **LeakyReLU**: like ReLU but allows small negative slope\n",
      "metadata": {}
    },
    {
      "id": "97113448",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def make_activation(name: str) -> nn.Module:\n    name = name.lower()\n    if name == \"relu\":\n        return nn.ReLU()\n    if name == \"tanh\":\n        return nn.Tanh()\n    if name == \"leakyrelu\":\n        return nn.LeakyReLU(negative_slope=0.01)\n    raise ValueError(\"activation must be one of: relu, tanh, leakyrelu\")\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim: int, hidden_sizes: List[int], activation: str, num_classes: int = 2, dropout: float = 0.0):\n        super().__init__()\n        layers = []\n        in_dim = input_dim\n        act = make_activation(activation)\n\n        for h in hidden_sizes:\n            layers.append(nn.Linear(in_dim, h))\n            layers.append(act)\n            if dropout and dropout > 0:\n                layers.append(nn.Dropout(dropout))\n            in_dim = h\n\n        layers.append(nn.Linear(in_dim, num_classes))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n",
      "outputs": []
    },
    {
      "id": "b3ae8e9e",
      "cell_type": "markdown",
      "source": "## 5) Training loop (Backprop happens here)\n\n### Why CrossEntropyLoss?\nFor classification, we output **logits** (raw scores).\n`CrossEntropyLoss` applies softmax internally and computes a stable loss.\n\n### Backprop key idea (course connection)\n- `loss.backward()` computes gradients: **\u2202Loss/\u2202weights**\n- `optimizer.step()` updates weights using those gradients (gradient descent variant)\n\nWe also track **gradient norm** as a simple signal of backprop magnitude.\n",
      "metadata": {}
    },
    {
      "id": "dddbd520",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "def grad_global_norm(model: nn.Module) -> float:\n    total = 0.0\n    for p in model.parameters():\n        if p.grad is None:\n            continue\n        total += p.grad.detach().data.norm(2).item() ** 2\n    return total ** 0.5\n\n\n@torch.no_grad()\ndef evaluate(model: nn.Module, loader: DataLoader, loss_fn: nn.Module) -> Tuple[float, float]:\n    model.eval()\n    loss_sum, acc_sum, n = 0.0, 0.0, 0\n    for Xb, yb in loader:\n        Xb, yb = Xb.to(device), yb.to(device)\n        logits = model(Xb)\n        loss = loss_fn(logits, yb)\n        loss_sum += loss.item()\n        acc_sum += accuracy_from_logits(logits, yb)\n        n += 1\n    return loss_sum / max(1, n), acc_sum / max(1, n)\n\n\ndef train_model(\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    lr: float = 1e-3,\n    epochs: int = 50,\n    weight_decay: float = 0.0,\n    save_path: str = \"best_model.pt\"\n) -> Tuple[Dict[str, List[float]], Dict[str, Any]]:\n    model.to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": [], \"grad_norm\": []}\n    best = {\"val_acc\": -1.0, \"epoch\": -1}\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        loss_sum, acc_sum, grad_sum, n = 0.0, 0.0, 0.0, 0\n\n        for Xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n            Xb, yb = Xb.to(device), yb.to(device)\n\n            logits = model(Xb)\n            loss = loss_fn(logits, yb)\n\n            # ---- Backpropagation core ----\n            optimizer.zero_grad()\n            loss.backward()      # compute gradients\n            optimizer.step()     # update parameters\n            # ------------------------------\n\n            gnorm = grad_global_norm(model)\n\n            loss_sum += loss.item()\n            acc_sum += accuracy_from_logits(logits, yb)\n            grad_sum += gnorm\n            n += 1\n\n        train_loss = loss_sum / max(1, n)\n        train_acc = acc_sum / max(1, n)\n        avg_grad = grad_sum / max(1, n)\n\n        val_loss, val_acc = evaluate(model, val_loader, loss_fn)\n\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n        history[\"grad_norm\"].append(avg_grad)\n\n        # save best by validation accuracy\n        if val_acc > best[\"val_acc\"]:\n            best = {\"val_acc\": float(val_acc), \"epoch\": int(epoch)}\n            torch.save(model.state_dict(), save_path)\n\n    return history, {\"best\": best}\n",
      "outputs": []
    },
    {
      "id": "361504c8",
      "cell_type": "markdown",
      "source": "## 6) Baseline run\n\nWe start with a baseline model and visualize:\n- training curves\n- decision boundary on test set\n\nThis is your simplest \u201cit works\u201d run (technical implementation points).\n",
      "metadata": {}
    },
    {
      "id": "c3dcf7e1",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "os.makedirs(\"outputs/checkpoints\", exist_ok=True)\nos.makedirs(\"outputs/figures\", exist_ok=True)\n\nbaseline_cfg = {\"hidden_sizes\": [64, 64], \"activation\": \"relu\", \"dropout\": 0.0}\nbaseline_model = MLP(input_dim=2, hidden_sizes=baseline_cfg[\"hidden_sizes\"], activation=baseline_cfg[\"activation\"], dropout=baseline_cfg[\"dropout\"])\n\nbaseline_ckpt = \"outputs/checkpoints/best_baseline.pt\"\nhistory, meta = train_model(\n    baseline_model,\n    train_loader,\n    val_loader,\n    lr=1e-3,\n    epochs=50,\n    weight_decay=0.0,\n    save_path=baseline_ckpt\n)\n\nprint(\"Best epoch:\", meta[\"best\"][\"epoch\"], \"Best val acc:\", meta[\"best\"][\"val_acc\"])\nplot_history(history)\n\n# Load best and evaluate on test\nbaseline_model.load_state_dict(torch.load(baseline_ckpt, map_location=\"cpu\"))\ntest_loss, test_acc = evaluate(baseline_model, test_loader, nn.CrossEntropyLoss())\nprint(f\"Baseline Test Loss: {test_loss:.4f} | Baseline Test Acc: {test_acc:.4f}\")\n\ndecision_boundary(baseline_model, X_test, y_test, title=\"Baseline decision boundary (test set)\")\n",
      "outputs": []
    },
    {
      "id": "f89bda6b",
      "cell_type": "markdown",
      "source": "## 7) Experiments: architecture & activation\n\nTo show understanding of **model capacity** and **training dynamics**, we run a small experiment set:\n\n- Smaller network vs deeper network\n- ReLU vs Tanh vs LeakyReLU\n\nWe save:\n- model checkpoints\n- a CSV table of results\n- a simple plot comparing test accuracy\n",
      "metadata": {}
    },
    {
      "id": "28c86d2a",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import pandas as pd\n\nexperiment_defs = [\n    (\"small_1layer\", [16], \"relu\"),\n    (\"medium_2layer\", [32, 32], \"relu\"),\n    (\"bigger_3layer\", [64, 64, 64], \"relu\"),\n    (\"tanh_2layer\", [64, 64], \"tanh\"),\n    (\"leakyrelu_2layer\", [64, 64], \"leakyrelu\"),\n]\n\nresults = []\n\nfor name, hidden_sizes, activation in experiment_defs:\n    seed_everything(42)\n    model = MLP(input_dim=2, hidden_sizes=hidden_sizes, activation=activation, dropout=0.0)\n\n    ckpt = f\"outputs/checkpoints/{name}.pt\"\n    hist, meta = train_model(\n        model, train_loader, val_loader,\n        lr=1e-3,\n        epochs=40,\n        weight_decay=0.0,\n        save_path=ckpt\n    )\n\n    # evaluate best checkpoint on test\n    model.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n    t_loss, t_acc = evaluate(model, test_loader, nn.CrossEntropyLoss())\n\n    results.append({\n        \"name\": name,\n        \"hidden_sizes\": str(hidden_sizes),\n        \"activation\": activation,\n        \"best_val_acc\": meta[\"best\"][\"val_acc\"],\n        \"best_epoch\": meta[\"best\"][\"epoch\"],\n        \"test_acc\": t_acc,\n        \"test_loss\": t_loss,\n    })\n\ndf = pd.DataFrame(results).sort_values(\"test_acc\", ascending=False)\ndf\n",
      "outputs": []
    },
    {
      "id": "d5857b78",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Save results table\ncsv_path = \"outputs/experiment_results.csv\"\ndf.to_csv(csv_path, index=False)\nprint(\"Saved:\", csv_path)\n\n# Plot comparison\nplt.figure(figsize=(9, 4))\nplt.bar(df[\"name\"], df[\"test_acc\"])\nplt.xticks(rotation=45, ha=\"right\")\nplt.ylabel(\"test accuracy\")\nplt.title(\"Experiment comparison (test accuracy)\")\nplt.grid(True, axis=\"y\")\nplt.tight_layout()\nfig_path = \"outputs/figures/experiment_test_accuracy.png\"\nplt.savefig(fig_path, dpi=200)\nplt.show()\nprint(\"Saved:\", fig_path)\n",
      "outputs": []
    },
    {
      "id": "d1437d9b",
      "cell_type": "markdown",
      "source": "## 8) Critical analysis (write your interpretation here)\n\nAfter you run the experiments, write a short analysis (this is where you earn the **Critical Analysis** points).\n\nUse prompts like:\n\n- Which model performed best on the test set? Why might that be?\n- Did the deeper model ([64, 64, 64]) show better accuracy? Did it seem to overfit?\n- How did activations compare (ReLU vs Tanh vs LeakyReLU)?\n- What happened to gradient norms across epochs? What might that indicate about training stability?\n\n### Limitations / tradeoffs\nBe honest (professors like this):\n- Toy dataset is not \u201creal world\u201d\n- Limited hyperparameter tuning (learning rate/epochs)\n- Only accuracy reported (no calibration/robustness)\n\n### Possible extensions (optional)\n- add dropout / weight decay comparisons\n- try MNIST\n- try a learning rate scheduler\n",
      "metadata": {}
    },
    {
      "id": "8ad71779",
      "cell_type": "markdown",
      "source": "## 9) Peer review templates (paste your feedback)\n\n### Peer Review 1\n- **What worked well:**\n- **One improvement:**\n- **Question for the author:**\n\n### Peer Review 2\n- **What worked well:**\n- **One improvement:**\n- **Question for the author:**\n",
      "metadata": {}
    }
  ]
}